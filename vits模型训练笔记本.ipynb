{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luxiya0615/yezaikai/blob/main/vits%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%AC%94%E8%AE%B0%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forked from https://github.com/CjangCjengh/vits/blob/main/vits.ipynb\n",
        "\n",
        "该笔记本可用于训练vits单人和多人模型，不包括语音合成。\n",
        "\n",
        "**请注意该笔记本不包括合成语音，而且默认只保存一个checkpoint以节省空间。建议每次重新开始训练之前备份一次。**\n",
        "\n",
        "**默认每隔200次step保存一次，可在“每隔多少次step保存一次断点”部分进行修改。**\n",
        "\n",
        "**在看到进度save之前不要轻易退出，以免丢失进度。**\n",
        "\n",
        "[tacotron2笔记本](https://colab.research.google.com/drive/18fbCupSaQde-FtF2Z2Na-LP5BrukjNMs?usp=sharing)\n",
        "\n",
        "[添加情感向量支持的vits笔记本](https://colab.research.google.com/drive/10MkPCQhhTs30jwUSMpZ8mTbptqpUOLnl?usp=sharing)\n",
        "\n",
        "[单人数据集制作工具包](https://colab.research.google.com/drive/1oM3HuRdGtONgpNNTredRCYeG_JrdF1be?usp=sharing)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hw61j0aOOoNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 常见问题Q&A\n",
        "Q：压缩包解压后还是“FileNotFoundError”？\n",
        "\n",
        "确保压缩包下的文件夹**直接是**`wavs`和`filelists`。如果还是不行，请检查压缩包的格式是否在支持的格式范围内。\n",
        "\n",
        "Q: ValueError: too many values to unpack (expected 3)\n",
        "\n",
        "如果是训练单人模型，txt台词文件不需要加中间的序号，和tacotron2一样就可以了。\n",
        "\n",
        "如果是训练多人模型，台词中间不能出现符号'|'。\n",
        "\n",
        "Q: IndexError: list index out of range\n",
        "\n",
        "台词文件不能有任何空行。\n",
        "\n",
        "Q: UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8d in position 21: invalid start byte\n",
        "\n",
        "txt文件的编码问题，需要保存成UTF-8格式。\n",
        "\n",
        "Q: RuntimeError: shape '\\[1, 1, 264836\\]' is invalid for input of size 529672\n",
        "\n",
        "需要wav文件为**单声道**\n",
        "\n",
        "Q：CUDA out of memory\n",
        "\n",
        "~~喜报：CUDA out of memory~~\n",
        "\n",
        "batch_size设置太大或语音文件过长，可以尝试减小batch_size或缩短语音文件长度。\n",
        "\n",
        "Q：训练集多大效果比较好？训练多长时间？\n",
        "\n",
        "建议200条语音起步，最好2000条语音以上。训练到200 iteration左右，即出现\"ieration 200\"字样即可。\n",
        "\n",
        "另外收集的语音质量对效果影响比较大，建议收集发音清晰、情绪平稳的语音，音频文件格式参考tacotron2，音频时长在3\\~10秒左右。\n",
        "\n",
        "Q：我的数据集实在不够，有办法解决吗？\n",
        "\n",
        "如果一个角色的语音比较少，可以尝试先用**同一语种**的其他角色的语音混进去，等合成出的语音发音标准后再换数据集、只留一个角色的语音训练(可以理解为“微调”)。**注意配置中的speaker前后不能变**。\n",
        "\n",
        "Q: AssertionError: 4D tensors expect 4 values for padding\n",
        "\n",
        "报错原因是音频文件是立体声，导致多了一个维度。\n",
        "\n",
        "需要将音频文件转成22050Hz**单声道**的wav文件。\n",
        "\n",
        "Q：如何继续上次的进度训练？\n",
        "\n",
        "如果云端硬盘中存放模型的路径没有改变，使用上次运行的笔记本重新运行一遍即可。\n",
        "\n",
        "Q：我训练了很长时间，合成效果还是不太行，怎么办？\n",
        "\n",
        "可能是以下原因：\n",
        "1. 数据集有较多错误，台词和语音不对应\n",
        "2. 合成时或训练时cleaner选择错误，cleaner和symbol不对应\n",
        "3. 语音发音不清晰，语气激烈\n",
        "4. 语音过长或过短\n",
        "5. 样本数量太少\n",
        "\n",
        "Q: 有些角色因为语音比较少，效果不如其他角色，有什么好办法吗？\n",
        "\n",
        "可以尝试在台词txt文件中把这个角色的台词多复制粘贴几遍，比如：\n",
        "```\n",
        "wavs/001.wav|0|バラバラ\n",
        "wavs/002.wav|0|バラバラ\n",
        "```\n",
        "改成\n",
        "```\n",
        "wavs/001.wav|0|バラバラ\n",
        "wavs/002.wav|0|バラバラ\n",
        "wavs/001.wav|0|バラバラ\n",
        "wavs/002.wav|0|バラバラ\n",
        "wavs/001.wav|0|バラバラ\n",
        "wavs/002.wav|0|バラバラ\n",
        "```\n",
        "通过这种方式让该角色的语音被学习的次数更多一些，可能有效改善语音质量。\n",
        "\n",
        "### 其他相关参考\n",
        "\n",
        "[Tacotron2、Vits、SoVits、Diffsvc常见报错及其解决方案](https://www.bilibili.com/read/cv20636396)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UI7tNVeoz1DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 准备\n",
        "#@markdown 定义工具函数 `run_command` `run_command_by_line` `get_symbols` 和 `get_tensorboard_showing`\n",
        "# forked from https://www.endpointdev.com/blog/2015/01/getting-realtime-output-using-python/\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "def run_command(command_args):\n",
        "    def print_pipe(raw):\n",
        "        return print(raw.decode(\"utf-8\"), end='')\n",
        "    try:\n",
        "      process = subprocess.Popen(command_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "      out, err = process.communicate()\n",
        "    except:\n",
        "      pass\n",
        "    print_pipe(out)\n",
        "    print_pipe(err)\n",
        "    rc = process.poll()\n",
        "    return rc\n",
        "\n",
        "def run_command_by_line(command_args):\n",
        "    def print_pipe(raw):\n",
        "        return print(raw.decode(\"utf-8\"), end='')\n",
        "    with subprocess.Popen(command_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as process:\n",
        "      while process.poll() is None:\n",
        "        print_pipe(process.stdout.readline())\n",
        "      errlines = process.stderr.readlines()\n",
        "      errlines = [line.decode(\"utf-8\") for line in errlines]\n",
        "      if len(errlines) > 0:\n",
        "        sp = \"\\r\\n\"\n",
        "        print(f'Warning: {sp} {\"\".join(errlines)}')\n",
        "    return\n",
        "\n",
        "'''\n",
        "Defines the set of symbols used in text input to the model.\n",
        "'''\n",
        "\n",
        "symbols_map = {\n",
        "    \"japanese_cleaners\": {\n",
        "        \"_pad\": '_',\n",
        "        \"_punctuation\": ',.!?-',\n",
        "        \"_letters\": 'AEINOQUabdefghijkmnoprstuvwyzʃʧ↓↑ '\n",
        "    },\n",
        "    \"japanese_cleaners2\": {\n",
        "        \"_pad\": '_',\n",
        "        \"_punctuation\": ',.!?-~…',\n",
        "        \"_letters\": 'AEINOQUabdefghijkmnoprstuvwyzʃʧʦ↓↑ ',\n",
        "    },\n",
        "    \"korean_cleaners\": {\n",
        "        \"_pad\": '_',\n",
        "        \"_punctuation\": ',.!?…~',\n",
        "        \"_letters\": 'ㄱㄴㄷㄹㅁㅂㅅㅇㅈㅊㅋㅌㅍㅎㄲㄸㅃㅆㅉㅏㅓㅗㅜㅡㅣㅐㅔ ',\n",
        "    },\n",
        "    \"cjke_cleaners2\": {\n",
        "        \"_pad\": '_',\n",
        "        \"_punctuation\": ',.!?…~',\n",
        "        \"_letters\": 'NQabdefghijklmnopstuvwxyzɑæʃʑçɯɪɔɛɹðəɫɥɸʊɾʒθβŋɦ⁼ʰ`',\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def get_symbols(specify_cleaners):\n",
        "    if re.match(r'english_cleaners', specify_cleaners):\n",
        "        specify_cleaners = \"cjke_cleaners2\"\n",
        "    if specify_cleaners not in symbols_map.keys():\n",
        "        raise ValueError(\"不存在对应cleaners的symbols!\")\n",
        "    symbols = symbols_map[specify_cleaners]\n",
        "    return [symbols[\"_pad\"]] + list(symbols[\"_punctuation\"]) + list(symbols[\"_letters\"])\n",
        "\n",
        "def get_tensorboard_showing(logdir):\n",
        "    from multiprocessing import Process\n",
        "    from tensorboard import notebook\n",
        "    import tensorflow as tf\n",
        "    import time\n",
        "\n",
        "    def run_tb():\n",
        "        run_command_by_line([\"tensorboard\",\"--reload_interval\", \"30\",  \"--logdir\", logdir, \"--bind_all\"])\n",
        "\n",
        "    def monitor_tb():\n",
        "        while True:\n",
        "            try:\n",
        "                notebook.display(height=998)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                time.sleep(3)\n",
        "\n",
        "    Process(target=run_tb).start()\n",
        "    Process(target=monitor_tb).start()\n",
        "\n",
        "def clean_empty_lines(file):\n",
        "    file_in = open(file, \"r\", encoding=\"utf-8\")\n",
        "    content = file_in.readlines()\n",
        "    file_in.close()\n",
        "    file_out = open(file, \"w\", encoding=\"utf-8\")\n",
        "    for i in range(len(content)):\n",
        "        line = content[i]\n",
        "        line = line.strip()\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        if i == len(content) - 1:\n",
        "            print(line, file=file_out, end=\"\")\n",
        "        else:\n",
        "            print(line, file=file_out)\n",
        "    file_out.close()"
      ],
      "metadata": {
        "id": "SaxypO5jwX__",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOgjdsQgKTfD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 加载Google云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 使用缓存数据（可选）\n",
        "#@markdown 如果之前缓存过数据，可填入缓存数据的压缩包路径，跳过\"下载依赖库\"( 除安装依赖外 )至\"预处理\"的步骤 <br />\n",
        "#@markdown 否则可以跳过\n",
        "hh_cache_data_use_path = \"/content/drive/MyDrive/cache.7z\" # @param {type:\"string\"}\n",
        "run_command_by_line([\"7z\", \"x\", hh_cache_data_use_path, \"vits\"])\n",
        "os.chdir('/content/vits')\n",
        "!pip install -r requirements.txt\n",
        "!sudo apt-get install espeak -y\n",
        "!sudo apt-get install p7zip-full p7zip-rar\n",
        "!pip install demjson\n",
        "!pip install transformers"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YSqd1XTq9gKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_0vZ-OjHVNu",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 下载依赖库\n",
        "#@markdown 取消勾选则不会节省空间\n",
        "colab_save_space = True #@param {type:\"boolean\"}\n",
        "os.chdir('/content')\n",
        "run_command_by_line([\"git\", \"clone\", \"https://github.com/wind4000/vits.git\", \"-b\", \"save-space-2\" if colab_save_space else \"main\"])\n",
        "os.chdir('/content/vits')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 安装依赖\n",
        "os.chdir('/content/vits')\n",
        "!rm /usr/local/bin/cmake\n",
        "!cmake --version\n",
        "!pip install -r requirements.txt\n",
        "!sudo apt-get install espeak -y\n",
        "!sudo apt-get install p7zip-full p7zip-rar\n",
        "!pip install setuptools==57.5\n",
        "!pip install demjson\n",
        "!pip install -U numpy==1.23.0"
      ],
      "metadata": {
        "id": "0_QcDFLYHrH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3a-FsHghwXS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 解压数据集\n",
        "#@markdown 压缩包路径\n",
        "import subprocess\n",
        "dataset_path = \"/content/drive/MyDrive/dataset/YOURDATASET.rar\"  #@param {type:\"string\"}\n",
        "os.chdir('/content/vits')\n",
        "run_command_by_line([\"7z\", \"x\", dataset_path])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**目前支持的cleaner(和tacotron2版效果不同)**\n",
        "\n",
        "cleaners from https://github.com/CjangCjengh/vits\n",
        "\n",
        "english cleaners 来自 `cjke_cleaners2` in https://github.com/CjangCjengh/vits\n",
        "\n",
        "|序号|cleaners名称|语种|\n",
        "|---|---|---|\n",
        "|1. |japanese_cleaners|日语|\n",
        "|2. |korean_cleaners|韩语|\n",
        "|3. |english_cleaners_ipa|英语|\n",
        "|4. |english_cleaners_ipa2|英语|\n",
        "|5. |english_cleaners_lazy_ipa|英语|"
      ],
      "metadata": {
        "id": "nsiO9Zr0TTtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 生成配置文件\n",
        "# forked from https://github.com/CjangCjengh/vits/blob/main/configs/japanese_ss_base2.json\n",
        "#@markdown 配置文件名称\n",
        "json_filename = \"test.json\" #@param {type:\"string\"}\n",
        "#@markdown 训练次数\n",
        "hparams_epochs = 2000 #@param {type:\"integer\"}\n",
        "#@markdown 每隔多少次step保存一次断点\n",
        "hparams_eval_interval = 200 #@param {type:\"integer\"}\n",
        "#@markdown 单次step的文件数（建议在16以内）\n",
        "hparams_batch_size = 12 #@param {type:\"integer\"}\n",
        "#@markdown 训练集文件列表\n",
        "hparams_training_files = \"/content/vits/filelists/list.txt\" #@param {type:\"string\"}\n",
        "#@markdown 验证集文件列表\n",
        "hparams_validation_files = \"/content/vits/filelists/list.txt\"#@param {type:\"string\"}\n",
        "#@markdown 选择cleaner\n",
        "hparams_cleaner =  \"japanese_cleaners\" #@param {type:\"string\"}\n",
        "#@markdown 人物名，多个人物用英文逗号隔开\n",
        "hparams_speaker = \"test\" #@param {type:\"string\"}\n",
        "#@markdown 模型名\n",
        "hparams_model_name = \"test\" #@param {type:\"string\"}\n",
        "\n",
        "hparams_symbols = get_symbols(hparams_cleaner)\n",
        "speakers = [speaker.strip() for speaker in hparams_speaker.split(\",\")]\n",
        "print(\"speakers: \")\n",
        "for i, speaker in enumerate(speakers):\n",
        "  print(\"\\t{a}: {b}\".format(a=i, b=speaker))\n",
        "training_json = {\n",
        "  \"train\": {\n",
        "    \"log_interval\": 200,\n",
        "    \"eval_interval\": hparams_eval_interval,\n",
        "    \"seed\": 1234 ,\n",
        "    \"epochs\": hparams_epochs,\n",
        "    \"learning_rate\": 2e-4,\n",
        "    \"betas\": [0.8, 0.99],\n",
        "    \"eps\": 1e-9,\n",
        "    \"batch_size\": hparams_batch_size,\n",
        "    \"fp16_run\": True,\n",
        "    \"lr_decay\": 0.999875,\n",
        "    \"segment_size\": 8192,\n",
        "    \"init_lr_ratio\": 1,\n",
        "    \"warmup_epochs\": 0,\n",
        "    \"c_mel\": 45,\n",
        "    \"c_kl\": 1.0\n",
        "  },\n",
        "  \"data\": {\n",
        "    \"training_files\": hparams_training_files + \".cleaned\",\n",
        "    \"validation_files\": hparams_validation_files + \".cleaned\",\n",
        "    \"text_cleaners\":[hparams_cleaner],\n",
        "    \"max_wav_value\": 32768.0,\n",
        "    \"sampling_rate\": 22050,\n",
        "    \"filter_length\": 1024,\n",
        "    \"hop_length\": 256,\n",
        "    \"win_length\": 1024,\n",
        "    \"n_mel_channels\": 80,\n",
        "    \"mel_fmin\": 0.0,\n",
        "    \"mel_fmax\": None,\n",
        "    \"add_blank\": True,\n",
        "    \"n_speakers\": len(speakers) if len(speakers) > 1 else 0,\n",
        "    \"cleaned_text\": True\n",
        "  },\n",
        "  \"model\": {\n",
        "    \"inter_channels\": 192,\n",
        "    \"hidden_channels\": 192,\n",
        "    \"filter_channels\": 768,\n",
        "    \"n_heads\": 2,\n",
        "    \"n_layers\": 6,\n",
        "    \"kernel_size\": 3,\n",
        "    \"p_dropout\": 0.1,\n",
        "    \"resblock\": \"1\",\n",
        "    \"resblock_kernel_sizes\": [3,7,11],\n",
        "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
        "    \"upsample_rates\": [8,8,2,2],\n",
        "    \"upsample_initial_channel\": 512,\n",
        "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
        "    \"n_layers_q\": 3,\n",
        "    \"use_spectral_norm\": False,\n",
        "  },\n",
        "  \"speakers\": speakers,\n",
        "  \"symbols\": hparams_symbols\n",
        "}\n",
        "\n",
        "if len(speakers) > 1:\n",
        "  training_json[\"model\"][\"gin_channels\"] = 256\n",
        "\n",
        "import demjson\n",
        "os.chdir('/content/vits/configs')\n",
        "training_json_text = demjson.encode(training_json)\n",
        "with open(json_filename, \"w\") as file:\n",
        "  file.write(training_json_text)\n",
        "\n",
        "os.chdir('/content/vits/text')\n",
        "with open(\"symbols.py\", \"w\") as file:\n",
        "  print(\"symbols = \", hparams_symbols, sep=\"\", file=file)\n",
        "os.chdir('/content/vits')\n",
        "\n"
      ],
      "metadata": {
        "id": "b0P9X9SSPl0L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOsV22D8IUTS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 预处理\n",
        "#@markdown 尝试自动清空行\n",
        "is_auto_clean_empty = False # @param {type:\"boolean\"}\n",
        "if is_auto_clean_empty:\n",
        "  print(f'---0. Clean empty line...---')\n",
        "  clean_empty_lines(hparams_training_files)\n",
        "  clean_empty_lines(hparams_validation_files)\n",
        "print(f'---1. Preprocess...---')\n",
        "os.chdir('/content/vits/monotonic_align')\n",
        "!python setup.py build_ext --inplace\n",
        "os.chdir('/content/vits')\n",
        "run_command([\"python\", \"preprocess.py\", \"--text_index\", \"2\" if len(speakers) > 1 else \"1\", \"--text_cleaners\", hparams_cleaner, \"--filelists\", hparams_training_files, hparams_validation_files])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 缓存数据（可选）\n",
        "#@markdown **缓存后可省去一些繁琐步骤，但会占用很多空间， 可跳过**\n",
        "%cd /content\n",
        "hh_cache_data_path = \"/content/drive/MyDrive/cache.7z\" # @param {type:\"string\"}\n",
        "run_command_by_line([\"7z\", \"a\", hh_cache_data_path, \"vits\"])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ff25PbjR9kOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltU2JXpxIh-K",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 训练\n",
        "\n",
        "#@markdown 模型保存位置 <br >\n",
        "model_path = \"/content/drive/MyDrive\" # @param {type:\"string\"}\n",
        "#@markdown 启用tensorboard可视化数据\n",
        "enable_tb = False  # @param {type:\"boolean\"}\n",
        "if enable_tb:\n",
        "  logdir = os.path.join(model_path, hparams_model_name)\n",
        "  get_tensorboard_showing(logdir)\n",
        "os.chdir('/content/vits')\n",
        "run_command_by_line([\"python\", \"train_ms.py\" if len(speakers) > 1 else \"train.py\", \"-c\", \"configs/{json}\".format(json=json_filename), \"-m\", hparams_model_name, \"-o\", model_path])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文件目录结构如下 <br >\n",
        "```\n",
        "--模型保存位置 (model_path)\n",
        "---模型名 (hparams_model_name)\n",
        "----logs\n",
        "----G.pth\n",
        "----D.pth\n",
        "```\n",
        "\n",
        "例: 模型保存位置 `model_path`为 `/content/drive/MyDrive`，模型名 `hparams_model_name` 为 `test`\n",
        "\n",
        "则把模型放到 `/content/drive/MyDrive/test/G.pth` 和·`/content/drive/MyDrive/test/D.pth`"
      ],
      "metadata": {
        "id": "O6eOWpJ-BiwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 工具"
      ],
      "metadata": {
        "id": "HH-1FgeSGGLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这部分辅助[MoeTTS](https://github.com/luoyily/MoeTTS)等软件用vits合成语音。\n",
        "\n",
        "运行本部分前必须执行的步骤：“准备”、“下载依赖库”、“加载Google云端硬盘”和“生成配置文件”。\n",
        "\n",
        "这部分代码不要求GPU，可使用非GPU运行时，即达到限额后仍可使用。"
      ],
      "metadata": {
        "id": "gSkJ-OniKTUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 生成供MoeTTS使用的配置文件\n",
        "#@markdown 保存路径\n",
        "moetts_savepath = \"/content/drive/MyDrive/\" #@param {type:\"string\"}\n",
        "moetts_filepath = moetts_savepath + \"config.json\"\n",
        "moetts_filepath_symbol = moetts_savepath + \"moetts.json\"\n",
        "training_json[\"data\"][\"text_cleaners\"] = [\"custom_cleaners\"]\n",
        "training_json_text = demjson.encode(training_json)\n",
        "moetts_symbols = {\"symbols\": hparams_symbols}\n",
        "moetts_symbols_text = demjson.encode(moetts_symbols)\n",
        "with open(moetts_filepath, \"w\") as file:\n",
        "  file.write(training_json_text)\n",
        "with open(moetts_filepath_symbol, \"w\") as file:\n",
        "  file.write(moetts_symbols_text)\n",
        "print(\"已保存到\", moetts_filepath)\n",
        "print(\"已保存到\", moetts_filepath_symbol)"
      ],
      "metadata": {
        "id": "hnnXDR-KdYLt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 合成前转换文本\n",
        "os.chdir('/content/vits')\n",
        "import text\n",
        "input_text = \"\\u3053\\u308C\\u304B\\u3089\\u3082\\u3001\\u304A\\u308C\\u305F\\u3061\\u304C\\u305F\\u3061\\u3068\\u307E\\u3089\\u306A\\u3044\\u304B\\u304E\\u308A\\u3001\\u9053\\u306F\\u7D9A\\u304F\\u3002\" #@param {type:\"string\"}\n",
        "input_cleaners = \"japanese_cleaners\" #@param {type:\"string\"}\n",
        "try:\n",
        "  output_text = text._clean_text(input_text, [input_cleaners])\n",
        "  print(\"转换结果：\", output_text)\n",
        "except Exception as e:\n",
        "  print(\"文本有误？\", e)"
      ],
      "metadata": {
        "id": "2pjw4sIfGadz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}